# ğŸ¤– Multimodal RAG Chatbot with OCR, Vector Search, and Gemini Integration

A powerful **Streamlit-based AI chatbot** that performs **multimodal Retrieval-Augmented Generation (RAG)** by combining the following:
- ğŸ§  **Google Gemini LLM**
- ğŸ§¾ **Mistral OCR API**
- ğŸ“„ **PDF, DOCX, TXT, Image** ingestion
- ğŸ” **Vector Search using Pinecone**
- ğŸ’¬ **Chat Memory with Airtable**
- ğŸ“¸ **Multimodal inputs with deep embedding + OCR pipeline**

> ğŸŒ Live Demo: [Streamlit App](https://rag-chatbot-n3m296qczrmhwobmgy2yew.streamlit.app/)

---

## ğŸ“¸ Interface Screenshot

![Chatbot Screenshot](https://drive.google.com/uc?id=1VxOMtXx67shcMxFjKisRazK8QKJYn5sV)

---

## ğŸ—ï¸ System Architecture (Step-by-Step)

1. **User Interface (Streamlit):**  
   Users interact with a clean, chat-style interface to upload files and ask questions.

2. **Document Upload Handling:**  
   - Accepts `.pdf`, `.docx`, `.txt`, `.jpg`, `.png` files.  
   - Image or scanned files are processed using **Mistral OCR**.  
   - Text-based files are loaded using **LangChain loaders**.

3. **Text Chunking:**  
   Documents are split into smaller overlapping chunks using `RecursiveCharacterTextSplitter`.

4. **Embedding Generation:**  
   Each chunk is embedded using `sentence-transformers` (`all-MiniLM-L6-v2` model).

5. **Vector Storage in Pinecone:**  
   Embeddings and corresponding chunks are stored in a **Pinecone** vector index.

6. **Chat Input from User:**  
   Users enter a prompt/question in the chat input box.

7. **Contextual Retrieval (RAG):**  
   If RAG is enabled:
   - The prompt is embedded.
   - Pinecone retrieves top-k semantically similar chunks as context.

8. **Prompt Construction:**  
   Combines:
   - Retrieved context
   - Chat history from Airtable
   - Raw uploaded text

9. **Gemini LLM Response:**  
   The full prompt is sent to **Google Gemini** (via `google-generativeai`).  
   The LLM generates a context-aware answer.

10. **Output & Storage:**  
    - The assistant's reply is displayed in the chat.  
    - The interaction (prompt, reply, metadata, embedding) is saved to **Airtable**.

---

## ğŸ“Œ Key Features

- ğŸ” **RAG Pipeline**: Combine vector search with context-aware Gemini LLM.
- ğŸ“‘ **Document Support**: Upload and extract text from `.pdf`, `.docx`, `.txt`, `.jpg`, `.png`.
- ğŸ§  **Mistral OCR**: OCR processing for scanned images or PDFs.
- ğŸ“š **Vector Store with Pinecone**: Efficient semantic chunk search.
- ğŸ§¾ **Airtable Chat Memory**: Persistent, structured storage of messages with embeddings.
- ğŸ” **Conversation Threading**: Session-based multi-turn conversation history.
- ğŸ§ª **Model Selector**: Switch Gemini model versions on the fly.
- ğŸ¯ **Lightweight & Modular**: Easy to extend with new models or tools.

---

## âš™ï¸ Tech Stack

- `streamlit` â€” Web app UI
- `google-generativeai` â€” Gemini LLM APIs
- `mistralai` â€” OCR APIs from Mistral
- `pyairtable` â€” Airtable for memory and logging
- `pinecone-client` â€” Vector DB for semantic search
- `sentence-transformers` â€” Embedding generation
- `langchain` â€” Chunking, Doc parsing, schema

---
